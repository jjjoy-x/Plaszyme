import os
import torch
import pandas as pd
import itertools
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns

from plastic.mol_features.descriptors_rdkit import PlasticFeaturizer

# ========== ç”¨æˆ·é…ç½® ==========
SDF_DIR = "/Users/shulei/PycharmProjects/Plaszyme/plastic/mols_for_unimol_10_sdf"  # ä¿®æ”¹ä¸ºä½ çš„SDFæ–‡ä»¶å¤¹
CONFIG_PATH = "/Users/shulei/PycharmProjects/Plaszyme/plastic/mol_features/rdkit_features.yaml"
CO_MATRIX_CSV = "/Users/shulei/PycharmProjects/Plaszyme/test/outputs/plastic_co_matrix.csv"
SIM_THRESHOLD = 0.01
TEST_SIZE = 0.3
RANDOM_STATE = 42
OUTPUT_DIR = "run/ml_results_2"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ========== ç‰¹å¾æå– ==========
print("ğŸ§¬ æå–å¡‘æ–™ç»“æ„æè¿°ç¬¦...")
featurizer = PlasticFeaturizer(CONFIG_PATH)
feature_dict, _ = featurizer.featurize_folder(SDF_DIR)

features_df = pd.DataFrame.from_dict(feature_dict, orient="index")
features_df = features_df.sort_index()

# ========== åŠ è½½å…±é™è§£çŸ©é˜µ ==========
co_matrix = pd.read_csv(CO_MATRIX_CSV, index_col=0)
plastics = features_df.index.intersection(co_matrix.index)
features_df = features_df.loc[plastics]
co_matrix = co_matrix.loc[plastics, plastics]

# ========== æ„å»ºæ ·æœ¬å¯¹ï¼ˆäºŒåˆ†ç±»ä»»åŠ¡ï¼‰==========
X, y = [], []
pairs = list(itertools.combinations(plastics, 2))
for p1, p2 in pairs:
    if pd.isna(co_matrix.loc[p1, p2]):
        continue
    label = 1 if co_matrix.loc[p1, p2] >= SIM_THRESHOLD else 0
    pair_feature = np.abs(features_df.loc[p1] - features_df.loc[p2])  # å¯æ”¹ä¸ºæ‹¼æ¥æˆ–å…¶ä»–æ–¹å¼
    X.append(pair_feature.values)
    y.append(label)

X = np.array(X)
y = np.array(y)
feature_names = features_df.columns.tolist()

print(f"âœ… æ ·æœ¬å¯¹æ•°ï¼š{len(X)} (æ­£æ ·æœ¬: {sum(y)}, è´Ÿæ ·æœ¬: {len(y) - sum(y)})")

# ========== æ•°æ®åˆ’åˆ† ==========
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y)

# ========== æ¨¡å‹æ„å»º ==========
models = {
    "RandomForest": RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=RANDOM_STATE),
    "LogisticRegression": LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)
}

for name, model in models.items():
    print(f"\nğŸš€ è®­ç»ƒæ¨¡å‹ï¼š{name}")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]

    print(f"\nğŸ“Š åˆ†ç±»æŠ¥å‘Šï¼ˆ{name}ï¼‰:")
    print(classification_report(y_test, y_pred, digits=4))
    auc = roc_auc_score(y_test, y_proba)
    print(f"AUC: {auc:.4f}")

    # ç‰¹å¾é‡è¦æ€§
    if hasattr(model, "feature_importances_"):
        importances = model.feature_importances_
    elif hasattr(model, "coef_"):
        importances = np.abs(model.coef_[0])
    else:
        continue

    importance_df = pd.DataFrame({
        "feature": feature_names,
        "importance": importances
    }).sort_values("importance", ascending=False)

    importance_df.to_csv(os.path.join(OUTPUT_DIR, f"{name}_importance.csv"), index=False)

    # å¯è§†åŒ–
    plt.figure(figsize=(10, 5))
    sns.barplot(x="importance", y="feature", data=importance_df.head(15), palette="viridis")
    plt.title(f"Top 15 Feature Importance - {name}")
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, f"{name}_top_features.png"))
    plt.close()

print("âœ… æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œç»“æœä¿å­˜åœ¨ï¼š", OUTPUT_DIR)