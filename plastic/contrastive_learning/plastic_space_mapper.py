#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
infer_plastic_similarity_from_sdf.py

ç”¨é€”ï¼š
  è¯»å–ä¸€ä¸ª SDF ç›®å½•ï¼Œåˆ©ç”¨å·²è®­ç»ƒå¥½çš„å­ªç”Ÿç½‘ç»œï¼ˆ.ptï¼‰æŠŠæ‰€æœ‰å¡‘æ–™ç»“æ„æ˜ å°„åˆ°åµŒå…¥ç©ºé—´ï¼Œ
  è¾“å‡ºï¼šä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µã€ï¼ˆå¯é€‰ï¼‰æ¬§æ°è·ç¦»çŸ©é˜µã€åµŒå…¥å‘é‡ã€é™ç»´åæ ‡å’Œæ•£ç‚¹å›¾ã€‚

æ³¨æ„ï¼š
  - æ¨¡å‹çš„ç¬¬ä¸€å±‚è¾“å…¥ç»´åº¦å¿…é¡»ä¸è¿™é‡Œæå–çš„ RDKit ç‰¹å¾ç»´åº¦ä¸€è‡´ã€‚
  - è®­ç»ƒæ—¶è‹¥åšè¿‡ç‰¹å¾å½’ä¸€åŒ–ï¼Œè¿™é‡Œæœ€å¥½ç”¨ç›¸åŒç­–ç•¥ï¼ˆzscore / minmax / noneï¼‰ã€‚
"""

import os
import sys
import math
import json
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# ---- UMAP å¯é€‰ä¾èµ–ï¼ˆåªåœ¨é€‰æ‹© umap æ—¶éœ€è¦ï¼‰
UMAP_AVAILABLE = True
try:
    import umap  # type: ignore
except Exception:
    UMAP_AVAILABLE = False

# --------- ä½ çš„ RDKit ç‰¹å¾æå–å™¨ ----------
# éœ€è¦ä½ çš„é¡¹ç›®é‡Œå·²æœ‰è¯¥æ¨¡å—
from plastic.mol_features.descriptors_rdkit import PlasticFeaturizer


# ===================== ç”¨æˆ·é…ç½® =====================
RUN_NAME   = "runs/infer_from_sdf_7"

SDF_DIR    = "/Users/shulei/PycharmProjects/Plaszyme/plastic/mols_for_unimol_10_sdf_new"  # åŒ…å«å¤šä¸ª .sdf æ–‡ä»¶çš„ç›®å½•
CONFIG_YAML= "/path/to/plastic/mol_features/rdkit_features.yaml"  # RDKit ç‰¹å¾é…ç½®

MODEL_PT   = "/Users/shulei/PycharmProjects/Plaszyme/run/run_from_sdf_7/siamese_model.pt"  # è®­ç»ƒå¥½çš„æƒé‡ï¼ˆstate_dict æˆ–å®Œæ•´æ¨¡å‹ï¼‰
DEVICE     = "cuda" if torch.cuda.is_available() else "cpu"

# ä¸è®­ç»ƒä¸€è‡´çš„ç‰¹å¾å½’ä¸€åŒ–
NORMALIZE  = True
NORM_METHOD= "zscore"    # "zscore" | "minmax" | "none"

# ç›¸ä¼¼åº¦/è·ç¦»è¾“å‡º
SAVE_COSINE_SIMILARITY = True    # ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆ-1 ~ 1ï¼‰
SAVE_EUCLIDEAN_DISTANCE= True    # æ¬§æ°è·ç¦»ï¼ˆ>=0ï¼‰

# é™ç»´å¯è§†åŒ–
REDUCTION_METHOD = "tsne"        # "pca" | "umap" | "tsne"
RANDOM_STATE     = 42

# ===================== è‡ªåŠ¨è·¯å¾„ç®¡ç† =====================
OUTDIR = RUN_NAME
os.makedirs(OUTDIR, exist_ok=True)
FEATURES_CSV        = os.path.join(OUTDIR, "features.csv")
EMBEDDINGS_CSV      = os.path.join(OUTDIR, "embeddings.csv")
EMBEDDINGS_NPY      = os.path.join(OUTDIR, "embeddings.npy")
SIM_CSV             = os.path.join(OUTDIR, "plastic_similarity__cosine.csv")
DIST_CSV            = os.path.join(OUTDIR, "plastic_distance__euclidean.csv")
REDUCED_CSV         = os.path.join(OUTDIR, f"reduced_{REDUCTION_METHOD}.csv")
SCATTER_PNG         = os.path.join(OUTDIR, f"scatter_{REDUCTION_METHOD}.png")
INFO_PATH           = os.path.join(OUTDIR, "run_info.json")


# ===================== æ¨¡å‹ç»“æ„ï¼ˆéœ€ä¸è®­ç»ƒä¸€è‡´ï¼‰ =====================
class SiameseRegressor(nn.Module):
    def __init__(self, input_dim: int):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64)
        )
    def forward_once(self, x):
        return self.encoder(x)


# ===================== å·¥å…·å‡½æ•° =====================
def log(msg: str):
    print(msg, flush=True)

def save_info(payload: dict):
    with open(INFO_PATH, "w") as f:
        json.dump(payload, f, indent=2, ensure_ascii=False)

def load_or_featurize_sdf(sdf_dir: str, config_yaml: str) -> pd.DataFrame:
    """
    ä½¿ç”¨ä½ é¡¹ç›®é‡Œçš„ PlasticFeaturizer ä» SDF ç›®å½•æç‰¹å¾ï¼Œå¹¶ä¿å­˜åˆ° FEATURES_CSVã€‚
    """
    log("ğŸ§ª å¼€å§‹ä» SDF æå– RDKit ç‰¹å¾ ...")
    featurizer = PlasticFeaturizer(config_yaml)
    feature_dict, stats = featurizer.featurize_folder(sdf_dir)
    # è‡ªåŠ¨ä¿å­˜ï¼ˆå¸¦è¡¨å¤´ï¼‰
    prefix = FEATURES_CSV.replace(".csv", "")
    featurizer.save_features(feature_dict, prefix)
    df = pd.read_csv(FEATURES_CSV, index_col=0)
    log(f"âœ… æå–å®Œæˆï¼š{df.shape[0]} ä¸ªæ ·æœ¬ï¼Œ{df.shape[1]} ä¸ªç‰¹å¾ â†’ {FEATURES_CSV}")
    return df

def maybe_normalize(df: pd.DataFrame) -> pd.DataFrame:
    if not NORMALIZE or NORM_METHOD == "none":
        return df
    log(f"ğŸ”§ ç‰¹å¾å½’ä¸€åŒ–ï¼š{NORM_METHOD}")
    out = df.copy()
    if NORM_METHOD == "zscore":
        out[:] = StandardScaler().fit_transform(out)
    elif NORM_METHOD == "minmax":
        out[:] = MinMaxScaler().fit_transform(out)
    else:
        raise ValueError(f"Unknown NORM_METHOD: {NORM_METHOD}")
    return out

def find_first_linear_in_shape(state_dict: dict) -> int:
    """
    ä» state_dict çŒœç¬¬ä¸€å±‚ Linear çš„è¾“å…¥ç»´åº¦ã€‚
    å¸¸è§ key: 'encoder.0.weight' æˆ– 'module.encoder.0.weight'
    """
    # ä¼˜å…ˆæ‰¾åŒ…å« 'encoder.0.weight'
    for k, v in state_dict.items():
        if k.endswith("encoder.0.weight") or "encoder.0.weight" in k:
            return int(v.shape[1])
    # é€€è€Œæ±‚å…¶æ¬¡ï¼šæ‰¾ shape ç±»ä¼¼ [128, in_dim] çš„ç¬¬ä¸€ä¸ª weight
    for k, v in state_dict.items():
        if isinstance(v, torch.Tensor) and v.ndim == 2 and v.shape[0] == 128:
            return int(v.shape[1])
    raise RuntimeError("æ— æ³•ä» state_dict æ¨æ–­ç¬¬ä¸€å±‚è¾“å…¥ç»´åº¦ï¼ˆencoder.0.weight æœªæ‰¾åˆ°ï¼‰ã€‚")

def load_model(model_pt: str, in_dim: int | None, device: str) -> SiameseRegressor:
    ckpt = torch.load(model_pt, map_location=device)
    # å¯èƒ½æ˜¯ state_dict æˆ–å®Œæ•´æ¨¡å‹
    if isinstance(ckpt, nn.Module):
        model = ckpt
        model.to(device)
        model.eval()
        # å°è¯•ä»æ¨¡å‹ç¬¬ä¸€å±‚è¯»è¾“å…¥ç»´åº¦ï¼ˆå¯é€‰ï¼‰
        return model
    elif isinstance(ckpt, dict):
        # æœ‰äº›ä¿å­˜ä¸º {"state_dict": ...}
        sd = ckpt.get("state_dict", ckpt)
        expected_in = find_first_linear_in_shape(sd)
        if in_dim is not None and in_dim != expected_in:
            raise RuntimeError(f"âŒ ç‰¹å¾ç»´åº¦({in_dim}) ä¸æ¨¡å‹æœŸæœ›è¾“å…¥ç»´åº¦({expected_in})ä¸ä¸€è‡´ã€‚"
                               f"è¯·ç¡®ä¿ featurizer é…ç½®ä¸è®­ç»ƒæ—¶ä¸€è‡´ã€‚")
        model = SiameseRegressor(input_dim=expected_in).to(device)
        model.load_state_dict(sd, strict=False)
        model.eval()
        return model
    else:
        raise RuntimeError("æ— æ³•è¯†åˆ«çš„æ¨¡å‹æ–‡ä»¶æ ¼å¼ï¼ˆæ—¢ä¸æ˜¯ nn.Module ä¹Ÿä¸æ˜¯ state_dictï¼‰ã€‚")

@torch.no_grad()
def compute_embeddings(model: nn.Module, feats: pd.DataFrame, device: str) -> np.ndarray:
    X = torch.tensor(feats.values, dtype=torch.float32, device=device)
    Z = model.encoder(X) if hasattr(model, "encoder") else model(X)
    return Z.detach().cpu().numpy()

def cosine_similarity_matrix(Z: np.ndarray) -> pd.DataFrame:
    # å½’ä¸€åŒ–åç‚¹ä¹˜
    norms = np.linalg.norm(Z, axis=1, keepdims=True) + 1e-12
    Zhat = Z / norms
    S = Zhat @ Zhat.T
    return pd.DataFrame(S, index=names, columns=names)

def euclidean_distance_matrix(Z: np.ndarray) -> pd.DataFrame:
    # ||zi - zj||2
    # é«˜æ•ˆè®¡ç®—ï¼š||A-B||^2 = ||A||^2 + ||B||^2 - 2 AÂ·B
    G = Z @ Z.T
    sq = np.diag(G)
    D2 = sq[:, None] + sq[None, :] - 2 * G
    D2[D2 < 0] = 0.0
    D = np.sqrt(D2)
    return pd.DataFrame(D, index=names, columns=names)

def reduce_2d(Z: np.ndarray, method: str) -> np.ndarray:
    if method == "pca":
        return PCA(n_components=2).fit_transform(Z)
    if method == "tsne":
        return TSNE(n_components=2, random_state=RANDOM_STATE, perplexity=max(5, min(30, Z.shape[0]-1))).fit_transform(Z)
    if method == "umap":
        if not UMAP_AVAILABLE:
            raise RuntimeError("éœ€è¦å®‰è£… umap-learnï¼špip install umap-learn")
        return umap.UMAP(n_components=2, random_state=RANDOM_STATE).fit_transform(Z)
    raise ValueError(f"Unsupported REDUCTION_METHOD: {method}")

def plot_scatter(coords: np.ndarray, names: list[str], out_png: str, title: str):
    plt.figure(figsize=(7.2, 6))
    plt.scatter(coords[:, 0], coords[:, 1], s=18)
    for i, name in enumerate(names):
        plt.text(coords[i, 0], coords[i, 1], name, fontsize=7)
    plt.title(title)
    plt.tight_layout()
    plt.savefig(out_png, dpi=200)
    plt.close()


# ===================== ä¸»æµç¨‹ =====================
if __name__ == "__main__":
    log("=== Step 1/6: æå–æˆ–è½½å…¥ç‰¹å¾ ===")
    feats_df = load_or_featurize_sdf(SDF_DIR, CONFIG_YAML)
    names = feats_df.index.astype(str).tolist()
    in_dim = feats_df.shape[1]
    log(f"[INFO] ç‰¹å¾ç»´åº¦ = {in_dim}, æ ·æœ¬æ•° = {len(names)}")

    if NORMALIZE and NORM_METHOD != "none":
        feats_df = maybe_normalize(feats_df)

    log("=== Step 2/6: åŠ è½½æ¨¡å‹ ===")
    model = load_model(MODEL_PT, in_dim, DEVICE)
    # å¦‚æœæ¨¡å‹ state_dict æ¨æ–­çš„ç»´åº¦ä¸ df ä¸ä¸€è‡´ï¼Œä¸Šé¢çš„ load_model å·²ç»æŠ›é”™

    # è®°å½•å…³é”®ä¿¡æ¯
    info = {
        "run_name": RUN_NAME,
        "sdf_dir": SDF_DIR,
        "config_yaml": CONFIG_YAML,
        "model_pt": MODEL_PT,
        "device": DEVICE,
        "normalize": NORMALIZE,
        "norm_method": NORM_METHOD,
        "reduction_method": REDUCTION_METHOD,
        "n_samples": len(names),
        "n_features": in_dim,
    }
    save_info(info)

    log("=== Step 3/6: è®¡ç®—åµŒå…¥ ===")
    Z = compute_embeddings(model, feats_df, DEVICE)
    np.save(EMBEDDINGS_NPY, Z)
    pd.DataFrame(Z, index=names).to_csv(EMBEDDINGS_CSV)
    log(f"[OK] åµŒå…¥ä¿å­˜ï¼š{EMBEDDINGS_CSV} | {EMBEDDINGS_NPY} | å½¢çŠ¶={Z.shape}")

    log("=== Step 4/6: è®¡ç®—ç›¸ä¼¼åº¦/è·ç¦»çŸ©é˜µ ===")
    if SAVE_COSINE_SIMILARITY:
        sim_df = cosine_similarity_matrix(Z)
        sim_df.to_csv(SIM_CSV)
        log(f"[OK] ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µä¿å­˜ï¼š{SIM_CSV}  ï¼ˆèŒƒå›´çº¦ [-1, 1]ï¼Œå¯¹è§’=1.0ï¼‰")

    if SAVE_EUCLIDEAN_DISTANCE:
        dist_df = euclidean_distance_matrix(Z)
        dist_df.to_csv(DIST_CSV)
        log(f"[OK] æ¬§æ°è·ç¦»çŸ©é˜µä¿å­˜ï¼š{DIST_CSV}  ï¼ˆéè´Ÿï¼Œå¯¹è§’=0ï¼‰")

    log("=== Step 5/6: é™ç»´ä¸å¯è§†åŒ– ===")
    coords2d = reduce_2d(Z, REDUCTION_METHOD)
    pd.DataFrame(coords2d, index=names, columns=["x", "y"]).to_csv(REDUCED_CSV)
    plot_scatter(coords2d, names, SCATTER_PNG, f"{REDUCTION_METHOD.upper()} of embeddings (N={len(names)})")
    log(f"[OK] é™ç»´åæ ‡ï¼š{REDUCED_CSV}")
    log(f"[OK] æ•£ç‚¹å›¾ï¼š{SCATTER_PNG}")

    log("=== Step 6/6: å®Œæˆ ===")
    log(f"è¾“å‡ºç›®å½•ï¼š{OUTDIR}")